# ğŸš€ GuÃ­a de Despliegue en Digital Ocean

Esta guÃ­a te ayudarÃ¡ a desplegar **InfoPanama** (VerificaPty) en Digital Ocean.

## ğŸ“‹ Tabla de Contenidos

1. [Requisitos Previos](#requisitos-previos)
2. [Arquitectura de Despliegue](#arquitectura-de-despliegue)
3. [ConfiguraciÃ³n de Servicios](#configuraciÃ³n-de-servicios)
4. [Costos Estimados](#costos-estimados)
5. [Pasos de InstalaciÃ³n](#pasos-de-instalaciÃ³n)
6. [Variables de Entorno](#variables-de-entorno)
7. [AutomatizaciÃ³n con GitHub Actions](#automatizaciÃ³n-con-github-actions)
8. [Monitoreo y Logs](#monitoreo-y-logs)
9. [Troubleshooting](#troubleshooting)

---

## ğŸ¯ Requisitos Previos

- [ ] Cuenta en Digital Ocean ([Sign up aquÃ­](https://www.digitalocean.com))
- [ ] Cuenta en Vercel (para el frontend Next.js)
- [ ] Cuenta en Convex (para base de datos - ya configurado)
- [ ] Cuenta en Browserbase (para crawlers - ya configurado)
- [ ] Dominio personalizado (opcional pero recomendado)
- [ ] Git repository con el cÃ³digo

---

## ğŸ—ï¸ Arquitectura de Despliegue

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  USUARIOS                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VERCEL (Frontend - Next.js App)                â”‚
â”‚  - SSR & Static Generation                      â”‚
â”‚  - Edge Functions                                â”‚
â”‚  - CDN Global                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CONVEX (Backend - Database & API)              â”‚
â”‚  - Real-time Database                           â”‚
â”‚  - Queries & Mutations                          â”‚
â”‚  - File Storage                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DIGITAL OCEAN (Crawlers & Background Jobs)     â”‚
â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  App Platform (Web Service)            â”‚    â”‚
â”‚  â”‚  - Node.js 20                          â”‚    â”‚
â”‚  â”‚  - Auto-scaling                        â”‚    â”‚
â”‚  â”‚  - Health checks                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Cron Jobs / Workers                   â”‚    â”‚
â”‚  â”‚  - Crawlers automÃ¡ticos (3x/dÃ­a)      â”‚    â”‚
â”‚  â”‚  - Procesamiento de claims            â”‚    â”‚
â”‚  â”‚  - Limpieza de datos                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BROWSERBASE (Headless Browsers)                â”‚
â”‚  - Instagram scraping                           â”‚
â”‚  - Anti-detection                               â”‚
â”‚  - IP rotation                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ ConfiguraciÃ³n de Servicios

### 1ï¸âƒ£ Vercel (Frontend)

**Ya estÃ¡ configurado**, pero asegÃºrate de:

```bash
# Variables de entorno en Vercel Dashboard
NEXT_PUBLIC_CONVEX_URL=https://your-deployment.convex.cloud
NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=pk_live_...
CLERK_SECRET_KEY=sk_live_...
```

### 2ï¸âƒ£ Digital Ocean App Platform (Crawlers)

Vamos a desplegar los crawlers como un servicio en DO App Platform.

**CaracterÃ­sticas**:
- Auto-scaling basado en carga
- Health checks automÃ¡ticos
- Logs centralizados
- Rollback con un click

---

## ğŸ’° Costos Estimados

| Servicio | Plan | Costo Mensual |
|----------|------|---------------|
| **Vercel** | Pro | $20/mes (o Free para comenzar) |
| **Convex** | Professional | $25/mes (1M function calls) |
| **Digital Ocean App Platform** | Basic (512MB RAM) | $5/mes |
| **Browserbase** | Hobby | $20/mes |
| **Dominio** | .com | $12/aÃ±o (~$1/mes) |
| **TOTAL** | | **$70-71/mes** |

### Alternativa EconÃ³mica (para comenzar):
- Vercel: Free tier
- Convex: Free tier (hasta 1M llamadas)
- Digital Ocean: $5/mes
- Browserbase: $20/mes
- **TOTAL INICIAL: ~$25/mes**

---

## ğŸ“¦ Pasos de InstalaciÃ³n

### Paso 1: Preparar el Repositorio

1. AsegÃºrate de que todo estÃ© en Git:
```bash
git add .
git commit -m "feat: preparar para despliegue en Digital Ocean"
git push origin main
```

2. Crea un archivo `Dockerfile` en la raÃ­z del monorepo:

```dockerfile
# Ver archivo Dockerfile incluido abajo
```

3. Crea un archivo `.dockerignore`:

```
node_modules
.next
.turbo
*.log
.env.local
.DS_Store
```

### Paso 2: Crear App en Digital Ocean

1. Ve a [Digital Ocean App Platform](https://cloud.digitalocean.com/apps)

2. Click en **"Create App"**

3. **Source**:
   - Selecciona tu repositorio de GitHub
   - Autoriza a Digital Ocean a acceder a tu repo

4. **Resources**:
   - Type: **Worker** (para cron jobs)
   - Name: `infopanama-crawler`
   - Build Command: `npm install && npm run build --workspace=@infopanama/crawler`
   - Run Command: `npm run crawl:all --workspace=@infopanama/crawler`

5. **Environment Variables**:
   - Agrega todas las variables (ver secciÃ³n abajo)

6. **Pricing**:
   - Selecciona **Basic - $5/mes** (512MB RAM, suficiente para crawlers)

7. **Deploy**!

### Paso 3: Configurar Cron Job

Para ejecutar crawlers automÃ¡ticamente 3 veces al dÃ­a:

1. En Digital Ocean Dashboard â†’ Apps â†’ tu app
2. Ve a **Settings** â†’ **App-Level Cron Jobs**
3. Agrega:

```
# Ejecutar crawlers 3 veces al dÃ­a (8am, 2pm, 8pm UTC-5)
0 13,19,1 * * * npm run crawl:all --workspace=@infopanama/crawler
```

---

## ğŸ” Variables de Entorno

Configura estas variables en Digital Ocean App Platform:

```bash
# OpenAI (para extracciÃ³n de claims)
OPENAI_API_KEY=sk-proj-...

# Convex (base de datos)
CONVEX_URL=https://your-deployment.convex.cloud
NEXT_PUBLIC_CONVEX_URL=https://your-deployment.convex.cloud

# Browserbase (scraping avanzado)
BROWSERBASE_API_KEY=bb_live_...
BROWSERBASE_PROJECT_ID=...

# Node environment
NODE_ENV=production
```

---

## ğŸ¤– AutomatizaciÃ³n con GitHub Actions

Crea `.github/workflows/deploy.yml` para auto-deploy:

```yaml
name: Deploy to Digital Ocean

on:
  push:
    branches: [main]
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Install doctl
        uses: digitalocean/action-doctl@v2
        with:
          token: ${{ secrets.DIGITALOCEAN_ACCESS_TOKEN }}

      - name: Trigger deployment
        run: doctl apps create-deployment ${{ secrets.APP_ID }}
```

**ConfiguraciÃ³n en GitHub**:
1. Ve a Settings â†’ Secrets â†’ Actions
2. Agrega:
   - `DIGITALOCEAN_ACCESS_TOKEN`: Token de API de DO
   - `APP_ID`: ID de tu app (lo ves en la URL de DO)

---

## ğŸ“Š Monitoreo y Logs

### Ver Logs en Tiempo Real

```bash
# Instala doctl CLI
brew install doctl  # macOS
# o
sudo snap install doctl  # Linux

# AutentÃ­cate
doctl auth init

# Ver logs
doctl apps logs <APP_ID> --type=run --follow
```

### Monitoreo en Dashboard

1. Ve a Digital Ocean â†’ Apps â†’ tu app
2. PestaÃ±a **"Runtime Logs"**
3. Filtra por:
   - Tipo: `run` (para logs de ejecuciÃ³n)
   - PerÃ­odo: Ãºltimas 24h

### MÃ©tricas Importantes

- **CPU Usage**: DeberÃ­a estar <50% en promedio
- **Memory**: DeberÃ­a estar <400MB (de 512MB disponibles)
- **Restart Count**: DeberÃ­a ser 0 (si aumenta, hay problemas)

---

## ğŸ”§ Troubleshooting

### âŒ Error: "Build failed"

**Causa**: Dependencias faltantes o errores de compilaciÃ³n

**SoluciÃ³n**:
```bash
# Verifica que todo compile localmente
npm install
npm run build --workspace=@infopanama/crawler

# Si funciona local, revisa los logs de DO
doctl apps logs <APP_ID> --type=build
```

### âŒ Error: "Health check failed"

**Causa**: App no responde en el puerto correcto

**SoluciÃ³n**:
- Cambia el tipo de recurso de "Web Service" a "Worker"
- Los crawlers no necesitan exponer un puerto HTTP

### âŒ Crawlers no se ejecutan

**Causa**: Cron job mal configurado

**SoluciÃ³n**:
1. Verifica el cron syntax en [crontab.guru](https://crontab.guru)
2. AsegÃºrate de usar UTC timezone
3. Para PanamÃ¡ (UTC-5), suma 5 horas

Ejemplo: Para ejecutar a las 8am PanamÃ¡ = 13:00 UTC
```
0 13 * * *  # 8am PanamÃ¡ = 1pm UTC
```

### âŒ Out of Memory (OOM)

**Causa**: Crawlers usan mucha memoria

**SoluciÃ³n**:
1. Upgrade a plan de $12/mes (1GB RAM)
2. O optimiza los crawlers:
```typescript
// Limita crawlers simultÃ¡neos
const results = []
for (const crawler of crawlers) {
  const result = await crawler()  // Secuencial en vez de paralelo
  results.push(result)
}
```

---

## ğŸ¯ Checklist Final

Antes de dar por terminado el despliegue:

- [ ] Frontend desplegado en Vercel
- [ ] Crawlers desplegados en Digital Ocean
- [ ] Cron jobs configurados (3x/dÃ­a)
- [ ] Variables de entorno configuradas
- [ ] Browserbase configurado y funcionando
- [ ] Logs verificados (sin errores)
- [ ] GitHub Actions configurado (opcional)
- [ ] Dominio personalizado configurado (opcional)
- [ ] Monitoreo activo
- [ ] Primer scrape exitoso confirmado

---

## ğŸ“š Recursos Ãštiles

- [Digital Ocean App Platform Docs](https://docs.digitalocean.com/products/app-platform/)
- [doctl CLI Reference](https://docs.digitalocean.com/reference/doctl/)
- [Cron Expression Generator](https://crontab.guru)
- [Digital Ocean Community](https://www.digitalocean.com/community)

---

## ğŸ†˜ Soporte

Si encuentras problemas:

1. **Revisa los logs primero**: `doctl apps logs <APP_ID> --follow`
2. **Consulta esta guÃ­a**: Troubleshooting section
3. **Contacta al equipo**: Crea un issue en GitHub

---

**Â¡Listo!** ğŸ‰ Tu plataforma de fact-checking estarÃ¡ corriendo 24/7 en Digital Ocean.
