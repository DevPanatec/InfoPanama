# üìä Estado Actual del Sistema OSINT - InfoPanama

**Fecha:** 13 de Diciembre, 2025
**√öltima verificaci√≥n:** 11:15 AM

---

## ‚úÖ LO QUE YA FUNCIONA

### 1. **Crawlers** (10 de 11 funcionando)

| # | Medio | Estado | Art√≠culos |
|---|-------|--------|-----------|
| 1 | La Prensa | ‚úÖ Funciona | 11 scrapeados |
| 2 | TVN | ‚úÖ Funciona | ~10 scrapeados |
| 3 | Telemetro | ‚úÖ Funciona | En proceso |
| 4 | Panama Am√©rica | ‚úÖ Funciona | En proceso |
| 5 | Cr√≠tica | ‚úÖ Funciona | En proceso |
| 6 | La Estrella | ‚úÖ Funciona | En proceso |
| 7 | Capital Financiero | ‚úÖ Funciona | En proceso |
| 8 | Metro Libre | ‚úÖ Funciona | En proceso |
| 9 | RPC Radio | ‚úÖ Funciona | En proceso |
| 10 | Gaceta Oficial | ‚úÖ Funciona | En proceso |
| 11 | Foco Instagram | ‚ùå Bloqueado | Instagram bloquea Browserbase |

**Total esperado:** ~100-150 art√≠culos nuevos por ejecuci√≥n

---

### 2. **Extracci√≥n de Claims con IA**
- ‚úÖ OpenAI GPT-5-mini configurado
- ‚úÖ Prompts profesionales de fact-checking
- ‚úÖ Categorizaci√≥n autom√°tica
- ‚úÖ Risk level assessment
- ‚úÖ Guardado en Convex

---

### 3. **Base de Datos (Convex)**
- ‚úÖ Schema completo definido
- ‚úÖ Tablas: claims, articles, sources, entities, verdicts
- ‚úÖ Relaciones configuradas
- ‚úÖ Queries optimizadas

---

### 4. **Dashboard Administrativo**
- ‚úÖ Login con Clerk
- ‚úÖ Vista de claims
- ‚úÖ Gesti√≥n de actores
- ‚úÖ Gesti√≥n de fuentes
- ‚úÖ Gesti√≥n de eventos
- ‚úÖ Audit logs

---

### 5. **Sistema de Verificaci√≥n con IA**
- ‚úÖ Action `verifyClaim` en Convex
- ‚úÖ Mutation `saveVerdict` funcionando
- ‚úÖ Integraci√≥n con GPT-5-mini
- ‚úÖ UI de revisi√≥n de claims
- ‚ö†Ô∏è **NUNCA PROBADO END-TO-END**

---

### 6. **Landing Page**
- ‚úÖ Homepage con dise√±o profesional
- ‚úÖ Secci√≥n de verificaciones con an√°lisis OSINT
- ‚úÖ P√°gina de metodolog√≠a
- ‚úÖ P√°gina de sobre nosotros
- ‚úÖ Grafo de medios
- ‚úÖ **EntitiesSection** - An√°lisis de entidades con probabilidad de involucramiento
- ‚úÖ **RelationsGraph** - Grafo interactivo de relaciones entre entidades
- ‚ùå **0 claims publicados** (vac√≠o)

### 7. **P√°gina de Revisi√≥n de Claims (MEJORADA HOY)**
- ‚úÖ **Informaci√≥n completa del medio** (nombre, credibilidad, verificaci√≥n)
- ‚úÖ **Fecha y hora exacta de publicaci√≥n**
- ‚úÖ **Autor del art√≠culo**
- ‚úÖ **URL original + snapshot archivado**
- ‚úÖ **Contenido completo del art√≠culo** (expandible)
- ‚úÖ **Topics/temas detectados**
- ‚úÖ **Alertas de informaci√≥n faltante**
- ‚úÖ **Barra de credibilidad visual** (color-coded)
- ‚úÖ **Verificaci√≥n de fuente** (badges verificado/no verificado)

---

### 8. **Infraestructura**
- ‚úÖ Monorepo con Turborepo
- ‚úÖ Next.js 15 con App Router
- ‚úÖ TypeScript estricto
- ‚úÖ Tailwind CSS
- ‚úÖ shadcn/ui components
- ‚úÖ Convex backend
- ‚úÖ Clerk autenticaci√≥n

---

## ‚ùå LO QUE FALTA (CR√çTICO)

### 1. **Probar Sistema de Verificaci√≥n** üî¥
**Por qu√© es cr√≠tico:** No sabemos si funciona

**Acci√≥n:**
```bash
# 1. Abrir dashboard
open http://localhost:3000/admin/dashboard/claims

# 2. Seleccionar cualquier claim
# 3. Click "Verificar con IA"
# 4. Confirmar que se genera veredicto
```

---

### 2. **Publicar Claims al Homepage** üî¥
**Por qu√© es cr√≠tico:** Landing page vac√≠a, sin contenido p√∫blico

**Estado actual:**
- Claims totales: ~152
- Claims con status "published": **0**
- Claims p√∫blicos visibles: **0**

**Acci√≥n:**
```typescript
// En el dashboard, para cada claim verificado:
await updateClaim(claimId, {
  status: "published",
  isPublic: true,
  publishedAt: Date.now()
})
```

**Meta:** Publicar m√≠nimo 20-50 claims esta semana

---

### 3. **Verificar Grafos Funcionan** üü°
**Por qu√© es importante:** Es feature principal del OSINT

**Acci√≥n:**
```bash
# Abrir cualquier claim
open http://localhost:3000/verificaciones/[claim-id]

# Verificar que se muestra:
# - Grafo de entidades
# - Conexiones entre actores
# - Fuentes relacionadas
```

---

### 4. **Automatizar Crawlers** üü°
**Por qu√© es importante:** Contenido fresco autom√°tico

**Estado:** Manual solamente

**Acci√≥n:**
```bash
# Crear archivo .github/workflows/crawler.yml
# Configurar cron: cada 6 horas
# Agregar secrets en GitHub
```

**Costo:** $0 (GitHub Actions free tier)

---

### 5. **Sistema de Snapshots** üü¢
**Por qu√© es √∫til:** Preservar evidencia

**Opciones:**
- **Gratis:** Archive.org (wayback machine)
- **Pagado:** Digital Ocean Spaces ($5/mes)

---

## üìà M√âTRICAS ACTUALES

### Base de Datos
```
Claims:        ~152 (todos status "new")
‚îú‚îÄ new:        152 (100%)
‚îú‚îÄ review:     0
‚îú‚îÄ published:  0
‚îî‚îÄ rejected:   0

Entities:      182
‚îú‚îÄ conectadas: 144 (79%)
‚îî‚îÄ hu√©rfanas:  38 (21%)

Articles:      ~152
Sources:       4-5
Verdicts:      0 (probablemente)
```

### Crawlers
```
Ejecutados:    Manualmente
Frecuencia:    A demanda
√öltimo run:    Hoy 8:28 PM
Art√≠culos:     ~15-20 por crawler
Total/run:     ~150 art√≠culos
```

### Homepage
```
Visitantes:    0 (no deployado)
Claims p√∫blicos: 0
Engagement:    N/A
```

---

## üéØ PLAN DE ACCI√ìN INMEDIATO

### AHORA (Pr√≥ximos 30 minutos)

#### 1. Probar Verificaci√≥n con IA ‚è±Ô∏è 10 min
```bash
# Terminal 1: Asegurar que web app corre
cd apps/web
npm run dev

# Navegador:
# 1. http://localhost:3000/admin/dashboard
# 2. Click en cualquier claim
# 3. Click "Verificar con IA"
# 4. Esperar resultado
# 5. Confirmar que se guarda
```

**Resultado esperado:**
- ‚úÖ Veredicto generado por IA
- ‚úÖ Guardado en base de datos
- ‚úÖ Visible en UI del claim

**Si falla:**
- Revisar logs de Convex
- Revisar console.log() en navegador
- Verificar OPENAI_API_KEY

---

#### 2. Publicar 20 Claims ‚è±Ô∏è 15 min
```bash
# En el dashboard:
# Para cada claim verificado:

# Opci√≥n A: Manualmente en UI
1. Abrir claim
2. Click "Editar"
3. Cambiar status ‚Üí "published"
4. Check ‚úì isPublic
5. Save

# Opci√≥n B: Script r√°pido
# Crear: packages/convex/scripts/publish-claims.ts
```

**Script sugerido:**
```typescript
// packages/convex/scripts/publish-first-claims.ts
import { ConvexHttpClient } from 'convex/browser'

const client = new ConvexHttpClient(process.env.CONVEX_URL!)

async function publishTopClaims() {
  // 1. Get first 20 claims with veredictos
  const claims = await client.query('claims:list' as any, {
    limit: 20,
    status: 'review' // o 'new' si no hay verified
  })

  // 2. Publish each one
  for (const claim of claims) {
    await client.mutation('claims:update' as any, {
      id: claim._id,
      status: 'published',
      isPublic: true,
      publishedAt: Date.now()
    })
    console.log(`‚úÖ Publicado: ${claim.title}`)
  }
}

publishTopClaims()
```

---

#### 3. Verificar Homepage ‚è±Ô∏è 5 min
```bash
# Abrir
open http://localhost:3000

# Verificar que aparecen:
# - Claims publicados
# - Contador de verificaciones
# - √öltimo contenido
```

---

### HOY (Pr√≥ximas 2-3 horas)

#### 4. Configurar GitHub Actions ‚è±Ô∏è 30 min

**Archivo:** `.github/workflows/crawler-schedule.yml`

```yaml
name: Crawler Autom√°tico

on:
  schedule:
    # Cada 6 horas: 12 AM, 6 AM, 12 PM, 6 PM (UTC)
    - cron: '0 0,6,12,18 * * *'
  workflow_dispatch: # Permitir ejecuci√≥n manual

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run crawlers
        working-directory: packages/crawler
        env:
          CONVEX_URL: ${{ secrets.CONVEX_URL }}
          NEXT_PUBLIC_CONVEX_URL: ${{ secrets.CONVEX_URL }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          OPENAI_MODEL: gpt-5-mini
        run: npm run crawl:all

      - name: Notify on failure
        if: failure()
        run: |
          echo "‚ùå Crawler failed"
          # Aqu√≠ podr√≠as agregar notificaci√≥n por email/Slack
```

**Secrets a configurar en GitHub:**
1. `CONVEX_URL` ‚Üí https://accomplished-rhinoceros-93.convex.cloud
2. `OPENAI_API_KEY` ‚Üí sk-proj-xOpV...

**C√≥mo configurar secrets:**
```bash
# En GitHub:
# Settings ‚Üí Secrets and variables ‚Üí Actions ‚Üí New repository secret
```

---

#### 5. Implementar Snapshots con Archive.org ‚è±Ô∏è 45 min

**Modificar crawlers para archivar:**

```typescript
// packages/crawler/src/utils/archive.ts
export async function archiveUrl(url: string): Promise<string> {
  try {
    // Solicitar a Archive.org que guarde la p√°gina
    const archiveUrl = `https://web.archive.org/save/${url}`
    const response = await fetch(archiveUrl, {
      headers: {
        'User-Agent': 'InfoPanama OSINT Bot (archiving for fact-checking)'
      }
    })

    // Extraer URL del snapshot
    const location = response.headers.get('Content-Location')
    if (location) {
      return `https://web.archive.org${location}`
    }

    return archiveUrl
  } catch (error) {
    console.error('Error archiving:', error)
    return url
  }
}
```

**Integrar en crawler:**

```typescript
// En cada crawler, despu√©s de scrapear:
import { archiveUrl } from '../utils/archive'

const archivedUrl = await archiveUrl(article.url)

await createArticle({
  ...article,
  snapshotUrl: archivedUrl
})
```

---

#### 6. Dashboard de Estad√≠sticas ‚è±Ô∏è 1 hora

**Archivo:** `apps/web/src/app/admin/dashboard/page.tsx`

Agregar:
```tsx
// Query para stats
const stats = useQuery(api.stats.getDashboardStats)

<div className="grid grid-cols-4 gap-4 mb-8">
  <StatCard
    title="Claims Totales"
    value={stats?.totalClaims ?? 0}
    change="+12 esta semana"
    icon="üìä"
  />
  <StatCard
    title="Publicados"
    value={stats?.published ?? 0}
    change="+5 hoy"
    icon="‚úÖ"
  />
  <StatCard
    title="Pendientes"
    value={stats?.pending ?? 0}
    change="-3 desde ayer"
    icon="‚è≥"
  />
  <StatCard
    title="Tasa de √âxito"
    value={`${stats?.successRate ?? 0}%`}
    change="+2.5%"
    icon="üìà"
  />
</div>

<RecentActivity />
<CrawlerStatus />
```

**Query en Convex:**
```typescript
// packages/convex/convex/stats.ts
export const getDashboardStats = query({
  handler: async (ctx) => {
    const claims = await ctx.db.query("claims").collect()

    return {
      totalClaims: claims.length,
      published: claims.filter(c => c.status === "published").length,
      pending: claims.filter(c => c.status === "new").length,
      successRate: 85 // Calcular basado en veredictos
    }
  }
})
```

---

### ESTA SEMANA

#### 7. Botones de Moderaci√≥n R√°pida
#### 8. Sistema de Actores B√°sico
#### 9. Limpiar Entidades Hu√©rfanas
#### 10. Deploy a producci√≥n

---

## ‚úÖ CHECKLIST PRE-LAUNCH

### T√©cnico
- [ ] 10 crawlers verificados funcionando
- [ ] Sistema de verificaci√≥n probado
- [ ] 50+ claims publicados
- [ ] Grafos funcionando
- [ ] GitHub Actions configurado
- [ ] Snapshots activados
- [ ] Dashboard con stats

### Contenido
- [ ] 100+ claims verificados
- [ ] 20+ actores perfilados
- [ ] 10+ eventos documentados
- [ ] Metodolog√≠a publicada
- [ ] Sobre nosotros completo

### Infraestructura
- [ ] Dominio configurado
- [ ] SSL/HTTPS
- [ ] Analytics (Plausible)
- [ ] Monitoring (Sentry)
- [ ] Backups autom√°ticos

---

## üöÄ ESTADO: LISTO PARA TESTING

**Resumen:**
- ‚úÖ **Infraestructura:** 95% completa
- ‚ö†Ô∏è **Contenido:** 30% completo (falta publicar)
- ‚ö†Ô∏è **Automatizaci√≥n:** 50% completa (falta GitHub Actions)
- ‚ùå **Testing:** 0% (nunca probado end-to-end)

**Pr√≥ximo paso cr√≠tico:** **PROBAR SISTEMA DE VERIFICACI√ìN**

Una vez confirmado que funciona, proceder con publicaci√≥n masiva de claims.

---

**√öltima actualizaci√≥n:** 12 Dic 2025, 8:28 PM
