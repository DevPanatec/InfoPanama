# üï∑Ô∏è Crawler de InfoPanama - Gu√≠a de Uso

El crawler de InfoPanama es un sistema automatizado que extrae noticias de medios paname√±os, identifica claims verificables usando IA (OpenAI GPT-5 mini), y los guarda autom√°ticamente en Convex para su verificaci√≥n.

## üìã Caracter√≠sticas

- ‚úÖ **Scraping automatizado** de m√∫ltiples fuentes de noticias paname√±as
- ‚úÖ **Extracci√≥n de claims con IA** usando OpenAI GPT-5 mini
- ‚úÖ **Guardado autom√°tico** en Convex
- ‚úÖ **Categorizaci√≥n inteligente** de claims por riesgo y tipo
- ‚úÖ **Rate limiting** para no saturar servidores
- ‚úÖ **Manejo de errores** robusto

## üéØ Fuentes Configuradas

1. **La Prensa** - https://www.prensa.com
   - Secciones: Pol√≠tica, Econom√≠a, Sociedad, Nacionales

2. **Gaceta Oficial** - Documentos gubernamentales oficiales

## üöÄ Instalaci√≥n

El crawler ya est√° instalado en el proyecto. Solo necesitas configurar tu API key de OpenAI:

```bash
# 1. Copia el archivo de ejemplo
cp .env.example .env.local

# 2. Edita .env.local y agrega tu OPENAI_API_KEY
OPENAI_API_KEY=sk-proj-tu-api-key-aqui
OPENAI_MODEL=gpt-5-mini
```

## üéÆ Uso

### Opci√≥n 1: Script Ejecutable (Recomendado)

**En Windows:**
```bash
# Crawler completo (todas las fuentes)
run-crawler.bat

# Solo La Prensa
run-crawler.bat prensa

# Solo Gaceta Oficial
run-crawler.bat gaceta
```

**En Linux/Mac:**
```bash
# Dar permisos de ejecuci√≥n (primera vez)
chmod +x run-crawler.sh

# Crawler completo
./run-crawler.sh

# Solo La Prensa
./run-crawler.sh prensa

# Solo Gaceta Oficial
./run-crawler.sh gaceta
```

### Opci√≥n 2: Comandos npm directos

```bash
cd packages/crawler

# Crawler completo
npm run crawl:all

# Solo La Prensa
npm run crawl:prensa

# Solo Gaceta Oficial
npm run crawl:gaceta

# Modo desarrollo (con hot reload)
npm run dev
```

## üîÑ Pipeline del Crawler

El crawler ejecuta 3 fases autom√°ticamente:

### Fase 1: Crawling de Noticias
```
üîç Visita los sitios web de noticias
üìÑ Extrae art√≠culos recientes (m√°ximo 5 por secci√≥n)
üíæ Parsea t√≠tulo, contenido, autor, fecha, imagen
```

**L√≠mites de scraping:**
- 5 art√≠culos por secci√≥n
- 2 segundos de espera entre requests
- Timeout de 30 segundos por p√°gina

### Fase 2: Extracci√≥n de Claims con IA
```
ü§ñ Analiza cada art√≠culo con OpenAI GPT-5 mini
üéØ Identifica afirmaciones verificables
üìä Categoriza por tipo y nivel de riesgo
‚úÖ Filtra claims con confianza > 60%
```

**Categor√≠as de claims:**
- `pol√≠tica` - Declaraciones pol√≠ticas
- `econom√≠a` - PIB, presupuestos, inflaci√≥n
- `salud` - CSS, hospitales, medicamentos
- `seguridad` - Criminalidad, polic√≠a
- `infraestructura` - Obras p√∫blicas
- `otros` - Educaci√≥n, medio ambiente, cultura

**Niveles de riesgo:**
- `LOW` - Informaci√≥n t√©cnica, bajo impacto
- `MEDIUM` - Datos sin verificar
- `HIGH` - Declaraciones controversiales
- `CRITICAL` - Informaci√≥n que podr√≠a causar p√°nico

### Fase 3: Guardado en Convex
```
üíæ Guarda art√≠culos en la tabla articles
üìù Crea claims verificables en la tabla claims
üè∑Ô∏è Asigna categor√≠as y tags autom√°ticamente
üî• Marca como featured si es de alto riesgo
```

## üìä Monitoreo

Despu√©s de ejecutar el crawler, ver√°s un resumen como este:

```
üéâ PIPELINE COMPLETADO
============================================================
üì∞ Art√≠culos scrapeados: 20
üîç Claims extra√≠dos: 15
‚è±Ô∏è  Tiempo total: 45.23s
============================================================

üí° Pr√≥ximos pasos:
1. Revisar los claims en http://localhost:3000/admin/dashboard
2. Aprobar claims para verificaci√≥n autom√°tica
3. Publicar verificaciones en el homepage
```

## ü§ñ Automatizaci√≥n

### Con Cron Jobs de Convex

Ya est√° configurado para ejecutarse cada 6 horas autom√°ticamente:

```typescript
// packages/convex/convex/crons.ts
crons.interval(
  'crawl-news',
  { hours: 6 },
  internal.crawlers.crawlAndExtract
)
```

El cron job registra el evento pero NO ejecuta Playwright (que no puede correr en Convex). Para automatizaci√≥n completa, usa GitHub Actions:

### Con GitHub Actions (Recomendado)

Crea `.github/workflows/crawler.yml`:

```yaml
name: Run News Crawler

on:
  schedule:
    # Ejecutar cada 6 horas
    - cron: '0 */6 * * *'
  workflow_dispatch: # Permite ejecutar manualmente

jobs:
  crawl:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - uses: actions/setup-node@v3
        with:
          node-version: '20'

      - name: Install dependencies
        run: |
          cd packages/crawler
          npm install

      - name: Install Playwright browsers
        run: |
          cd packages/crawler
          npx playwright install --with-deps chromium

      - name: Run crawler
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          CONVEX_URL: ${{ secrets.CONVEX_URL }}
        run: |
          cd packages/crawler
          npm run crawl:all
```

**Configurar secretos en GitHub:**
1. Ve a Settings > Secrets and variables > Actions
2. Agrega:
   - `OPENAI_API_KEY`: Tu API key de OpenAI
   - `CONVEX_URL`: Tu URL de Convex (de .env.local)

## üí∞ Costos Estimados

### OpenAI GPT-5 mini
- **Precio:** ~$0.00015 por 1K tokens de entrada
- **Art√≠culo promedio:** ~2K tokens
- **Claim extraction:** ~$0.0003 por art√≠culo

**Ejemplo:**
- 20 art√≠culos scraped cada 6 horas
- 4 ejecuciones al d√≠a = 80 art√≠culos/d√≠a
- Costo: 80 √ó $0.0003 = **$0.024/d√≠a** = **~$0.70/mes**

### Playwright (Gratis)
- GitHub Actions: 2,000 minutos/mes gratis
- Cada crawl: ~2-3 minutos
- 4 crawls/d√≠a √ó 30 d√≠as = 120 crawls = ~360 minutos/mes
- ‚úÖ **100% dentro del tier gratuito**

## üîß Configuraci√≥n Avanzada

### Agregar Nuevas Fuentes

Edita o crea un nuevo crawler en `packages/crawler/src/crawlers/`:

```typescript
// packages/crawler/src/crawlers/tu-medio.ts
import { chromium } from 'playwright'
import type { ScrapedArticle } from '../types/index.js'

export async function crawlTuMedio(): Promise<ScrapedArticle[]> {
  const browser = await chromium.launch({ headless: true })
  const page = await browser.newPage()

  await page.goto('https://tumedio.com')

  // Extrae art√≠culos...
  const articles = []

  await browser.close()
  return articles
}
```

Luego agr√©galo a `src/index.ts`:

```typescript
import { crawlTuMedio } from './crawlers/tu-medio.js'

// En la funci√≥n main()
const tuMedioArticles = await crawlTuMedio()
articles = [...articles, ...tuMedioArticles]
```

### Personalizar Extracci√≥n de Claims

Edita el prompt en `packages/crawler/src/processors/claim-extractor.ts`:

```typescript
const EXTRACTION_PROMPT = `Eres un experto analista...`
```

## üêõ Troubleshooting

### Error: "CONVEX_URL no est√° configurado"
```bash
# Verifica que .env.local existe
cat .env.local

# Debe contener:
NEXT_PUBLIC_CONVEX_URL=https://tu-deployment.convex.cloud
```

### Error: "OpenAI API key inv√°lido"
```bash
# Verifica tu API key en:
# https://platform.openai.com/api-keys

# Actualiza .env.local
OPENAI_API_KEY=sk-proj-tu-nueva-key
```

### Playwright no encuentra el browser
```bash
# Instala browsers de Playwright
cd packages/crawler
npx playwright install chromium
```

### Claims no aparecen en el admin
1. Verifica que Convex est√© corriendo: `npm run dev`
2. Revisa logs del crawler: busca mensajes de "‚úÖ Claim creado"
3. Verifica en Convex dashboard: https://dashboard.convex.dev

## üìö Referencias

- [Documentaci√≥n de Playwright](https://playwright.dev)
- [OpenAI API Reference](https://platform.openai.com/docs)
- [Convex Documentation](https://docs.convex.dev)
- [GitHub Actions](https://docs.github.com/actions)

## üÜò Soporte

Si tienes problemas:
1. Revisa los logs completos del crawler
2. Verifica que todas las dependencias est√°n instaladas
3. Confirma que las API keys son v√°lidas
4. Revisa los issues en el repositorio

---

**√öltima actualizaci√≥n:** Diciembre 2025
