# ğŸ•·ï¸ CÃ³mo Usar el Crawler de InfoPanama

## Â¿QuÃ© hace el crawler?

1. **Scrapea noticias** de La Prensa y Gaceta Oficial automÃ¡ticamente
2. **Extrae claims verificables** con IA (OpenAI GPT-5 mini)
3. **Los guarda en Convex** listos para verificar en el admin

## ğŸš€ Uso RÃ¡pido

### Windows
```cmd
run-crawler.bat
```

### Linux/Mac
```bash
chmod +x run-crawler.sh
./run-crawler.sh
```

## âœ¨ Eso es todo!

El crawler harÃ¡ TODO automÃ¡ticamente:
- âœ… Visita sitios de noticias
- âœ… Extrae artÃ­culos recientes
- âœ… Identifica claims con IA
- âœ… Los guarda en tu base de datos

## ğŸ“Š DespuÃ©s del crawl

1. Ve a http://localhost:3000/admin/dashboard
2. VerÃ¡s los nuevos claims en "Verificaciones Pendientes"
3. Revisa y aprueba los claims
4. Â¡Listo para publicar!

## â±ï¸ Â¿CuÃ¡nto tarda?

- **20-30 artÃ­culos** por ejecuciÃ³n
- **~2 minutos** de scraping
- **~3 minutos** de anÃ¡lisis con IA
- **Total: ~5 minutos**

## ğŸ’° Â¿CuÃ¡nto cuesta?

- **OpenAI:** ~$0.024 por ejecuciÃ³n (~80 artÃ­culos)
- **~$0.70/mes** si ejecutas 4 veces al dÃ­a

## ğŸ¤– AutomatizaciÃ³n

Ya estÃ¡ configurado para ejecutarse cada 6 horas automÃ¡ticamente con los cron jobs de Convex.

Para automatizaciÃ³n completa (incluido Playwright), mira [CRAWLER_SETUP.md](CRAWLER_SETUP.md) para configurar GitHub Actions.

## ğŸ†˜ Problemas?

### "CONVEX_URL no estÃ¡ configurado"
Verifica que `.env.local` tenga:
```
NEXT_PUBLIC_CONVEX_URL=https://tu-deployment.convex.cloud
```

### "OpenAI API key invÃ¡lido"
Actualiza tu API key en `.env.local`:
```
OPENAI_API_KEY=sk-proj-tu-key-aqui
```

### No veo los claims en el admin
1. Verifica que `npm run dev` estÃ© corriendo
2. Revisa los logs del crawler

---

ğŸ“š **DocumentaciÃ³n completa:** [CRAWLER_SETUP.md](CRAWLER_SETUP.md)
